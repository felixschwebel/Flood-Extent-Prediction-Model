{
 "cells": [
  {
   "cell_type": "code",
   "id": "e4f75dd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T14:34:13.519990Z",
     "start_time": "2025-07-18T13:41:46.902523Z"
    }
   },
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import date\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from pyproj import Transformer\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "API_ROOT = \"https://geoglows.ecmwf.int/api/v2\"\n",
    "LAYER_URL = \"https://livefeeds3.arcgis.com/arcgis/rest/services/GEOGLOWS/GlobalWaterModel_Medium/MapServer/0/query\"\n",
    "\n",
    "# Create a session pool for better connection management\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\"User-Agent\": \"geoglows-comid-fetcher/1.0\"})\n",
    "    return session\n",
    "\n",
    "# Thread-local session storage\n",
    "import threading\n",
    "_thread_local = threading.local()\n",
    "\n",
    "def get_session():\n",
    "    if not hasattr(_thread_local, 'session'):\n",
    "        _thread_local.session = create_session()\n",
    "    return _thread_local.session\n",
    "\n",
    "# -----------------------------\n",
    "# Enhanced request function for discharge data specifically\n",
    "# -----------------------------\n",
    "def safe_get_discharge_json(url, retries=5, base_delay=0.1, max_delay=5):\n",
    "    \"\"\"\n",
    "    Specialized function for discharge API with aggressive retry logic for 500 errors\n",
    "    \"\"\"\n",
    "    session = get_session()\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Add small random jitter to distribute requests\n",
    "            if attempt > 0:\n",
    "                jitter = random.uniform(0, base_delay)\n",
    "                time.sleep(jitter)\n",
    "\n",
    "            r = session.get(url, timeout=30)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                return r.json()\n",
    "            elif r.status_code == 500:\n",
    "                # 500 errors seem to be transient, retry with exponential backoff\n",
    "                if attempt < retries - 1:\n",
    "                    wait_time = min(base_delay * (2 ** attempt), max_delay)\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                r.raise_for_status()\n",
    "\n",
    "        except RequestException as e:\n",
    "            if attempt < retries - 1:\n",
    "                wait_time = min(base_delay * (2 ** attempt), max_delay)\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 1: Fetch COMID geometries (unchanged)\n",
    "# -----------------------------\n",
    "def fetch_comids_with_geometry(base_url, where_clause=\"1=1\", batch_size=2000, cache_file=\"comid_geometry_cache.json\"):\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\") as f:\n",
    "            print(f\"🔄 Loading COMID geometries from cache: {cache_file}\")\n",
    "            return json.load(f)\n",
    "\n",
    "    transformer = Transformer.from_crs(\"EPSG:3857\", \"EPSG:4326\", always_xy=True)\n",
    "    all_comids_with_geometry = {}\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"where\": where_clause,\n",
    "            \"outFields\": \"comid,streamorder\",\n",
    "            \"returnGeometry\": \"true\",\n",
    "            \"f\": \"json\",\n",
    "            \"resultOffset\": offset,\n",
    "            \"resultRecordCount\": batch_size,\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        features = data.get(\"features\", [])\n",
    "        if not features:\n",
    "            break\n",
    "\n",
    "        for feature in features:\n",
    "            attrs = feature.get(\"attributes\", {})\n",
    "            geometry = feature.get(\"geometry\", {})\n",
    "            comid = attrs.get(\"comid\")\n",
    "            streamorder = attrs.get(\"streamorder\")\n",
    "\n",
    "            if comid is not None and \"paths\" in geometry:\n",
    "                raw_path = geometry[\"paths\"][0]\n",
    "                # Transform from Web Mercator (EPSG:3857) to WGS84 (EPSG:4326)\n",
    "                # always_xy=True ensures transform returns (longitude, latitude)\n",
    "                latlon_path = [transformer.transform(x, y) for x, y in raw_path]\n",
    "                all_comids_with_geometry[comid] = {\n",
    "                    \"polyline\": latlon_path,\n",
    "                    \"streamorder\": streamorder,\n",
    "                }\n",
    "\n",
    "        offset += batch_size\n",
    "\n",
    "    with open(cache_file, \"w\") as f:\n",
    "        json.dump(all_comids_with_geometry, f)\n",
    "        print(f\"✅ Saved COMID geometries to cache: {cache_file}\")\n",
    "\n",
    "    # Debug: Check coordinate ranges for Nicaragua\n",
    "    if all_comids_with_geometry:\n",
    "        sample_comid = next(iter(all_comids_with_geometry))\n",
    "        sample_coords = all_comids_with_geometry[sample_comid][\"polyline\"]\n",
    "        lons = [coord[0] for coord in sample_coords]\n",
    "        lats = [coord[1] for coord in sample_coords]\n",
    "        print(f\"🔍 Coordinate check - Lon range: {min(lons):.2f} to {max(lons):.2f}, Lat range: {min(lats):.2f} to {max(lats):.2f}\")\n",
    "        print(f\"   Nicaragua should be roughly: Lon -87 to -83, Lat 10 to 15\")\n",
    "\n",
    "    return all_comids_with_geometry\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2: Keep return periods fetching as-is\n",
    "# -----------------------------\n",
    "def fetch_return_periods(comid_list, cache_file=\"return_periods_cache.json\", max_workers=100):\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\") as f:\n",
    "            print(f\"🔄 Loading return periods from cache: {cache_file}\")\n",
    "            return json.load(f)\n",
    "\n",
    "    def fetch_single_rp(comid):\n",
    "        session = get_session()\n",
    "        url = f\"{API_ROOT}/returnperiods/{comid}?format=json&bias_corrected=true\"\n",
    "        try:\n",
    "            r = session.get(url, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            js = r.json()\n",
    "            if \"return_periods\" in js:\n",
    "                return comid, js[\"return_periods\"]\n",
    "        except:\n",
    "            pass\n",
    "        return comid, None\n",
    "\n",
    "    rp_dict = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(fetch_single_rp, comid): comid for comid in comid_list}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Fetching return periods\"):\n",
    "            comid, data = fut.result()\n",
    "            if data:\n",
    "                rp_dict[comid] = data\n",
    "\n",
    "    with open(cache_file, \"w\") as f:\n",
    "        json.dump(rp_dict, f)\n",
    "        print(f\"✅ Saved return periods to cache: {cache_file}\")\n",
    "\n",
    "    return rp_dict\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3: Optimized discharge fetching\n",
    "# -----------------------------\n",
    "def get_retrospective_day_robust(river_id: int, day: str, rp_data: dict):\n",
    "    \"\"\"Robust discharge fetching with specialized error handling\"\"\"\n",
    "    try:\n",
    "        retro_url = f\"{API_ROOT}/retrospectivedaily/{river_id}?format=json&start_date={day}&end_date={day}&bias_corrected=true\"\n",
    "\n",
    "        js = safe_get_discharge_json(retro_url)\n",
    "\n",
    "        if not js:\n",
    "            return None\n",
    "\n",
    "        date_str = js['datetime'][0]\n",
    "        discharge = js[str(river_id)][0]\n",
    "        unit = js['metadata']['units']['long']\n",
    "\n",
    "        return_periods = rp_data.get(str(river_id)) or rp_data.get(int(river_id))\n",
    "        return_period = 0\n",
    "        if return_periods:\n",
    "            for rp, threshold in sorted(return_periods.items(), key=lambda x: x[1]):\n",
    "                if discharge >= threshold:\n",
    "                    return_period = int(rp)\n",
    "\n",
    "        return {\n",
    "            \"comid\": river_id,\n",
    "            \"date\": date_str,\n",
    "            \"discharge_m3s\": discharge,\n",
    "            \"unit\": unit,\n",
    "            \"exceeds_rp\": return_period\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Adaptive discharge fetching with failure recovery\n",
    "# -----------------------------\n",
    "def fetch_all_discharge_adaptive(comid_list, target_date, rp_data, initial_workers=80, min_workers=20):\n",
    "    \"\"\"\n",
    "    Adaptive approach: start with high concurrency, reduce if too many failures\n",
    "    \"\"\"\n",
    "    failed_comids = []\n",
    "    successful_results = []\n",
    "    current_workers = initial_workers\n",
    "\n",
    "    def process_batch(comids, workers):\n",
    "        results = []\n",
    "        failures = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(get_retrospective_day_robust, comid, target_date, rp_data): comid\n",
    "                for comid in comids\n",
    "            }\n",
    "\n",
    "            for fut in tqdm(as_completed(futures), total=len(futures),\n",
    "                          desc=f\"Discharge ({workers} workers)\"):\n",
    "                result = fut.result()\n",
    "                original_comid = futures[fut]\n",
    "\n",
    "                if result:\n",
    "                    results.append(result)\n",
    "                else:\n",
    "                    failures.append(original_comid)\n",
    "\n",
    "        return results, failures\n",
    "\n",
    "    # First attempt with high concurrency\n",
    "    print(f\"📈 First pass: trying {len(comid_list)} COMIDs with {current_workers} workers...\")\n",
    "    results, failures = process_batch(comid_list, current_workers)\n",
    "    successful_results.extend(results)\n",
    "\n",
    "    # Retry failed ones with reduced concurrency\n",
    "    retry_count = 1\n",
    "    max_retries = 5\n",
    "\n",
    "    while failures and retry_count <= max_retries:\n",
    "        current_workers = max(min_workers, current_workers // 2)\n",
    "        print(f\"🔄 Retry {retry_count}: {len(failures)} failed COMIDs with {current_workers} workers...\")\n",
    "\n",
    "        # Add some delay before retry\n",
    "        time.sleep(2)\n",
    "\n",
    "        retry_results, new_failures = process_batch(failures, current_workers)\n",
    "        successful_results.extend(retry_results)\n",
    "        failures = new_failures\n",
    "        retry_count += 1\n",
    "\n",
    "    if failures:\n",
    "        print(f\"⚠️ Final status: {len(failures)} COMIDs still failed after {max_retries} retries\")\n",
    "        print(f\"Failed COMIDs: {failures[:10]}{'...' if len(failures) > 10 else ''}\")\n",
    "\n",
    "    success_rate = len(successful_results) / len(comid_list) * 100\n",
    "    print(f\"✅ Overall success rate: {success_rate:.1f}% ({len(successful_results)}/{len(comid_list)})\")\n",
    "\n",
    "    return successful_results\n",
    "\n",
    "# -----------------------------\n",
    "# Alternative: Chunked processing for very large datasets\n",
    "# -----------------------------\n",
    "def fetch_all_discharge_chunked(comid_list, target_date, rp_data, chunk_size=500, max_workers=60):\n",
    "    \"\"\"\n",
    "    Process in chunks - good for very large datasets (6000+ COMIDs)\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    chunks = [comid_list[i:i + chunk_size] for i in range(0, len(comid_list), chunk_size)]\n",
    "\n",
    "    print(f\"📊 Processing {len(comid_list)} COMIDs in {len(chunks)} chunks of {chunk_size}\")\n",
    "\n",
    "    for chunk_num, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n🔄 Chunk {chunk_num}/{len(chunks)} ({len(chunk)} COMIDs)...\")\n",
    "\n",
    "        # Use adaptive approach for each chunk\n",
    "        chunk_results = fetch_all_discharge_adaptive(chunk, target_date, rp_data,\n",
    "                                                   initial_workers=max_workers, min_workers=20)\n",
    "        all_results.extend(chunk_results)\n",
    "\n",
    "        # Short break between chunks\n",
    "        if chunk_num < len(chunks):\n",
    "            time.sleep(1)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 5: Main execution with options\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    where_clause = \"rivercountry='Nicaragua'\"\n",
    "    target_date = \"20200315\"\n",
    "    # target_date = \"20201117\"\n",
    "\n",
    "    # Step 1: Get COMID geometries\n",
    "    print(\"🔍 Fetching geometries...\")\n",
    "    geometry_dict = fetch_comids_with_geometry(LAYER_URL, where_clause=where_clause)\n",
    "\n",
    "    # Use all COMIDs or limit for testing\n",
    "    comid_list = list(geometry_dict.keys())  # Remove [:100] to use all\n",
    "    print(f\"Found {len(comid_list)} COMIDs\")\n",
    "\n",
    "    # Step 2: Fetch return periods (keep high concurrency - this works fine)\n",
    "    print(f\"📊 Fetching return periods for {len(comid_list)} COMIDs...\")\n",
    "    rp_data = fetch_return_periods(comid_list, max_workers=100)\n",
    "\n",
    "    # Step 3: Choose your discharge fetching strategy:\n",
    "\n",
    "    if len(comid_list) <= 1000:\n",
    "        # For smaller datasets: use adaptive approach\n",
    "        print(f\"📈 Using adaptive approach for {len(comid_list)} COMIDs...\")\n",
    "        discharge_data = fetch_all_discharge_adaptive(comid_list, target_date, rp_data)\n",
    "    else:\n",
    "        # For large datasets (6000+): use chunked approach\n",
    "        print(f\"📈 Using chunked approach for {len(comid_list)} COMIDs...\")\n",
    "        discharge_data = fetch_all_discharge_chunked(comid_list, target_date, rp_data,\n",
    "                                                   chunk_size=500, max_workers=60)\n",
    "\n",
    "    # Step 4: Combine with geometry\n",
    "    combined_records = []\n",
    "    for item in discharge_data:\n",
    "        comid = item[\"comid\"]\n",
    "        if comid in geometry_dict:\n",
    "            line = geometry_dict[comid][\"polyline\"]\n",
    "            streamorder = geometry_dict[comid][\"streamorder\"]\n",
    "            geom = LineString(line)\n",
    "            record = {**item, \"geometry\": geom, \"streamorder\": streamorder}\n",
    "            combined_records.append(record)\n",
    "\n",
    "    # Step 5: Create GeoDataFrame and save\n",
    "    if combined_records:\n",
    "        gdf = gpd.GeoDataFrame(combined_records, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "        out_file = f\"nicaragua_discharge_{target_date}.geojson\"\n",
    "        gdf.to_file(out_file, driver=\"GeoJSON\")\n",
    "\n",
    "        final_success_rate = len(combined_records) / len(comid_list) * 100\n",
    "        print(f\"\\n🎉 FINAL RESULTS:\")\n",
    "        print(f\"✅ Saved {len(gdf)} features to {out_file}\")\n",
    "        print(f\"📊 Final success rate: {final_success_rate:.1f}% ({len(combined_records)}/{len(comid_list)})\")\n",
    "    else:\n",
    "        print(\"❌ No data retrieved successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Fetching geometries...\n",
      "🔄 Loading COMID geometries from cache: comid_geometry_cache.json\n",
      "Found 6592 COMIDs\n",
      "📊 Fetching return periods for 6592 COMIDs...\n",
      "🔄 Loading return periods from cache: return_periods_cache.json\n",
      "📈 Using chunked approach for 6592 COMIDs...\n",
      "📊 Processing 6592 COMIDs in 14 chunks of 500\n",
      "\n",
      "🔄 Chunk 1/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [02:36<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 110 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 110/110 [00:37<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 2/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [02:37<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 137 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 137/137 [00:47<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 3/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [02:45<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 163 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 163/163 [00:47<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 4/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [02:34<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 118 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 118/118 [01:04<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 2: 17 failed COMIDs with 20 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (20 workers): 100%|██████████| 17/17 [00:16<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 5/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [02:33<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 151 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 151/151 [00:52<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 2: 1 failed COMIDs with 20 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (20 workers): 100%|██████████| 1/1 [00:06<00:00,  6.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 6/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [02:46<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 130 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 130/130 [01:08<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 2: 30 failed COMIDs with 20 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (20 workers): 100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 7/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [02:13<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 151 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 151/151 [00:49<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 8/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [02:55<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 188 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 188/188 [01:29<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 2: 14 failed COMIDs with 20 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (20 workers): 100%|██████████| 14/14 [00:19<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 9/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [02:40<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 217 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 217/217 [01:29<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 2: 2 failed COMIDs with 20 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (20 workers): 100%|██████████| 2/2 [00:09<00:00,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 10/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [02:40<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 194 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 194/194 [01:09<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 2: 4 failed COMIDs with 20 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (20 workers): 100%|██████████| 4/4 [00:09<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 11/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [03:10<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 222 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 222/222 [01:10<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 12/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [03:07<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 173 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 173/173 [00:57<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 13/14 (500 COMIDs)...\n",
      "📈 First pass: trying 500 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 500/500 [02:45<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 148 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 148/148 [01:05<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 2: 3 failed COMIDs with 20 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (20 workers): 100%|██████████| 3/3 [00:09<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (500/500)\n",
      "\n",
      "🔄 Chunk 14/14 (92 COMIDs)...\n",
      "📈 First pass: trying 92 COMIDs with 60 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (60 workers): 100%|██████████| 92/92 [00:35<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Retry 1: 29 failed COMIDs with 30 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discharge (30 workers): 100%|██████████| 29/29 [00:19<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overall success rate: 100.0% (92/92)\n",
      "\n",
      "🎉 FINAL RESULTS:\n",
      "✅ Saved 6592 features to nicaragua_discharge_20200315.geojson\n",
      "📊 Final success rate: 100.0% (6592/6592)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T16:57:27.045754Z",
     "start_time": "2025-07-17T16:45:45.645427Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "f7583b2f5dc15bda",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "829668a4a564a015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "76449e88562403c2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climada_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
